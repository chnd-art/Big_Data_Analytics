{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "319cd063-b861-485a-b577-80e6c5a99f20",
   "metadata": {},
   "source": [
    "<h1 style=\"color: skyblue; text-align: center;\">Big Data Analytics</h1>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1763ee12-a643-4af2-a2b4-539f9d09c423",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<h3>1. Collecte de Données :</h3>\n",
    "           <p> On va utiliser le Dataset suivant</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2161142-033d-4087-bccb-7aaa9d9e0437",
   "metadata": {},
   "source": [
    " [Significant Earthquakes, 1965-2016](https://www.kaggle.com/datasets/usgs/earthquake-database/data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414f2807-7657-4635-b949-dac8014b600a",
   "metadata": {},
   "source": [
    "Puis on va copier le dataset dans HDFS sous le repertoire input "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c8c682-ab28-482a-9b8a-4ab5cfcfa5b0",
   "metadata": {},
   "source": [
    "<h3>2.Préparation des Données :</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2ba9815-d62e-446b-a898-c5decbb1776b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /usr/local/spark/python (3.2.4)\n",
      "Requirement already satisfied: py4j==0.10.9.5 in /usr/local/lib/python3.8/dist-packages (from pyspark) (0.10.9.5)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pyspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b6cccb5-4f04-4abb-a550-18f850be2a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "net.snowflake#snowflake-jdbc added as a dependency\n",
      "net.snowflake#spark-snowflake_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-83ca0419-1e6b-4ca5-a3cc-9914b1b19d90;1.0\n",
      "\tconfs: [default]\n",
      "\tfound net.snowflake#snowflake-jdbc;3.13.14 in central\n",
      "\tfound net.snowflake#spark-snowflake_2.12;2.10.0-spark_3.0 in central\n",
      "\tfound net.snowflake#snowflake-ingest-sdk;0.10.3 in central\n",
      ":: resolution report :: resolve 361ms :: artifacts dl 10ms\n",
      "\t:: modules in use:\n",
      "\tnet.snowflake#snowflake-ingest-sdk;0.10.3 from central in [default]\n",
      "\tnet.snowflake#snowflake-jdbc;3.13.14 from central in [default]\n",
      "\tnet.snowflake#spark-snowflake_2.12;2.10.0-spark_3.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-83ca0419-1e6b-4ca5-a3cc-9914b1b19d90\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/11ms)\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version: 3.2.4\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession  \n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"etl\") \\\n",
    "        .master(\"spark://hadoop-master:7077\") \\\n",
    "        .config(\"spark.jars.packages\", \"net.snowflake:snowflake-jdbc:3.13.14,net.snowflake:spark-snowflake_2.12:2.10.0-spark_3.0\") \\\n",
    "        .config(\"spark.executor.memory\", \"2g\") \\\n",
    "        .config(\"spark.driver.memory\", \"8g\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "\n",
    "print(\"Spark Version:\", spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79d6d8c3-57cb-4659-a145-6bc0c17cbed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(Date='01/02/1965', Time='13:44:18', Latitude=19.246, Longitude=145.616, Type='Earthquake', Depth=131.6, Depth Error=None, Depth Seismic Stations=None, Magnitude=6.0, Magnitude Type='MW', Magnitude Error=None, Magnitude Seismic Stations=None, Azimuthal Gap=None, Horizontal Distance=None, Horizontal Error=None, Root Mean Square=None, ID='ISCGEM860706', Source='ISCGEM', Location Source='ISCGEM', Magnitude Source='ISCGEM', Status='Automatic')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Charger le dataset \n",
    "df_load = spark.read.csv(\"hdfs:///user/root/input/database.csv\", header=True, inferSchema=True)\n",
    "# Aperçu de df_load\n",
    "df_load.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bf88480-4816-4554-8878-d28f029c7899",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+---------+----------+-----+---------+--------------+------------+\n",
      "|      Date|Latitude|Longitude|      Type|Depth|Magnitude|Magnitude Type|          ID|\n",
      "+----------+--------+---------+----------+-----+---------+--------------+------------+\n",
      "|01/02/1965|  19.246|  145.616|Earthquake|131.6|      6.0|            MW|ISCGEM860706|\n",
      "|01/04/1965|   1.863|  127.352|Earthquake| 80.0|      5.8|            MW|ISCGEM860737|\n",
      "|01/05/1965| -20.579| -173.972|Earthquake| 20.0|      6.2|            MW|ISCGEM860762|\n",
      "|01/08/1965| -59.076|  -23.557|Earthquake| 15.0|      5.8|            MW|ISCGEM860856|\n",
      "|01/09/1965|  11.938|  126.427|Earthquake| 15.0|      5.8|            MW|ISCGEM860890|\n",
      "+----------+--------+---------+----------+-----+---------+--------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Supprimer les champs dont nous n'avons pas besoin de df_load.\n",
    "lst_dropped_columns = ['Depth Error', 'Time', 'Depth Seismic Stations','Magnitude Error','Magnitude Seismic Stations','Azimuthal Gap',\n",
    "                       'Horizontal Distance','Horizontal Error',\n",
    "    'Root Mean Square','Source','Location Source','Magnitude Source','Status']\n",
    "\n",
    "df_load = df_load.drop(*lst_dropped_columns)\n",
    "# Aperçu df_load\n",
    "df_load.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "700d8cb7-cc8a-416d-896e-bb526ce0cadd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+---------+----------+-----+---------+--------------+------------+----+\n",
      "|      Date|Latitude|Longitude|      Type|Depth|Magnitude|Magnitude Type|          ID|Year|\n",
      "+----------+--------+---------+----------+-----+---------+--------------+------------+----+\n",
      "|01/02/1965|  19.246|  145.616|Earthquake|131.6|      6.0|            MW|ISCGEM860706|1965|\n",
      "|01/04/1965|   1.863|  127.352|Earthquake| 80.0|      5.8|            MW|ISCGEM860737|1965|\n",
      "|01/05/1965| -20.579| -173.972|Earthquake| 20.0|      6.2|            MW|ISCGEM860762|1965|\n",
      "|01/08/1965| -59.076|  -23.557|Earthquake| 15.0|      5.8|            MW|ISCGEM860856|1965|\n",
      "|01/09/1965|  11.938|  126.427|Earthquake| 15.0|      5.8|            MW|ISCGEM860890|1965|\n",
      "+----------+--------+---------+----------+-----+---------+--------------+------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Créer un champ année et l'ajouter au dataframe.\n",
    "df_load = df_load.withColumn('Year', year(to_timestamp('Date', 'dd/MM/yyyy')))\n",
    "# Aperçu df_load\n",
    "df_load.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f600ed4-229d-42dd-b002-6e2a9c1d49c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "|Year|Counts|\n",
      "+----+------+\n",
      "|1990|   196|\n",
      "|1975|   150|\n",
      "|1977|   148|\n",
      "|2003|   187|\n",
      "|2007|   211|\n",
      "+----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Construire le dataframe de fréquence en utilisant le champ année et les comptages pour chaque année.\n",
    "df_quake_freq = df_load.groupBy('Year').count().withColumnRenamed('count', 'Counts')\n",
    "# Aperçu df_quake_freq\n",
    "df_quake_freq.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75984474-df4f-4498-a4c5-b0da56a4dd8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Latitude: double (nullable = true)\n",
      " |-- Longitude: double (nullable = true)\n",
      " |-- Type: string (nullable = true)\n",
      " |-- Depth: double (nullable = true)\n",
      " |-- Magnitude: double (nullable = true)\n",
      " |-- Magnitude Type: string (nullable = true)\n",
      " |-- ID: string (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Aperçu df_load schema\n",
    "df_load.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc8781c9-f24b-481d-a442-196c8de49897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+---------+----------+-----+---------+--------------+------------+----+\n",
      "|      Date|Latitude|Longitude|      Type|Depth|Magnitude|Magnitude Type|          ID|Year|\n",
      "+----------+--------+---------+----------+-----+---------+--------------+------------+----+\n",
      "|01/02/1965|  19.246|  145.616|Earthquake|131.6|      6.0|            MW|ISCGEM860706|1965|\n",
      "|01/04/1965|   1.863|  127.352|Earthquake| 80.0|      5.8|            MW|ISCGEM860737|1965|\n",
      "|01/05/1965| -20.579| -173.972|Earthquake| 20.0|      6.2|            MW|ISCGEM860762|1965|\n",
      "|01/08/1965| -59.076|  -23.557|Earthquake| 15.0|      5.8|            MW|ISCGEM860856|1965|\n",
      "|01/09/1965|  11.938|  126.427|Earthquake| 15.0|      5.8|            MW|ISCGEM860890|1965|\n",
      "+----------+--------+---------+----------+-----+---------+--------------+------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convertir certains champs de chaîne de caractères en types numériques.\n",
    "df_load = df_load.withColumn('Latitude', df_load['Latitude'].cast(DoubleType()))\\\n",
    "    .withColumn('Longitude', df_load['Longitude'].cast(DoubleType()))\\\n",
    "    .withColumn('Depth', df_load['Depth'].cast(DoubleType()))\\\n",
    "    .withColumn('Magnitude', df_load['Magnitude'].cast(DoubleType()))\n",
    "\n",
    "# Aperçu df_load\n",
    "df_load.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56be19c6-687c-4168-9312-02938e5e869f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Latitude: double (nullable = true)\n",
      " |-- Longitude: double (nullable = true)\n",
      " |-- Type: string (nullable = true)\n",
      " |-- Depth: double (nullable = true)\n",
      " |-- Magnitude: double (nullable = true)\n",
      " |-- Magnitude Type: string (nullable = true)\n",
      " |-- ID: string (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Aperçu df_load schema\n",
    "df_load.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33e2530d-5329-4537-a2ca-f69d71a22a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer les champs de magnitude moyenne (avg magnitude) et de magnitude maximale (max magnitude) et les ajouter à df_quake_freq.\n",
    "df_max = df_load.groupBy('Year').max('Magnitude').withColumnRenamed('max(Magnitude)', 'Max_Magnitude')\n",
    "df_avg = df_load.groupBy('Year').avg('Magnitude').withColumnRenamed('avg(Magnitude)', 'Avg_Magnitude')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4d068c1-166f-421f-bf8e-96c89ace3a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-----------------+-------------+\n",
      "|Year|Counts|    Avg_Magnitude|Max_Magnitude|\n",
      "+----+------+-----------------+-------------+\n",
      "|1990|   196|5.858163265306125|          7.6|\n",
      "|1975|   150| 5.84866666666667|          7.8|\n",
      "|1977|   148|5.757432432432437|          7.6|\n",
      "|2003|   187|5.850802139037435|          7.6|\n",
      "|2007|   211| 5.89099526066351|          8.4|\n",
      "+----+------+-----------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Joindre df_max et df_avg à df_quake_freq.\n",
    "df_quake_freq = df_quake_freq.join(df_avg, ['Year']).join(df_max, ['Year'])\n",
    "# Aperçu df_quake_freq\n",
    "df_quake_freq.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70b338c7-4583-4eb7-99dc-75cf479e27a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Year: int, Counts: bigint, Avg_Magnitude: double, Max_Magnitude: double]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Supprimer les valeurs nulles.\n",
    "df_load.dropna()\n",
    "df_quake_freq.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71277b38-88f7-4029-b7d5-6047ecb4a09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+---------+----------+-----+---------+--------------+------------+----+\n",
      "|      Date|Latitude|Longitude|      Type|Depth|Magnitude|Magnitude Type|          ID|Year|\n",
      "+----------+--------+---------+----------+-----+---------+--------------+------------+----+\n",
      "|01/02/1965|  19.246|  145.616|Earthquake|131.6|      6.0|            MW|ISCGEM860706|1965|\n",
      "|01/04/1965|   1.863|  127.352|Earthquake| 80.0|      5.8|            MW|ISCGEM860737|1965|\n",
      "|01/05/1965| -20.579| -173.972|Earthquake| 20.0|      6.2|            MW|ISCGEM860762|1965|\n",
      "|01/08/1965| -59.076|  -23.557|Earthquake| 15.0|      5.8|            MW|ISCGEM860856|1965|\n",
      "|01/09/1965|  11.938|  126.427|Earthquake| 15.0|      5.8|            MW|ISCGEM860890|1965|\n",
      "+----------+--------+---------+----------+-----+---------+--------------+------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Aperçu dataframes\n",
    "df_load.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6c3f672-4e5f-42a6-9362-ae92f2141738",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-----------------+-------------+\n",
      "|Year|Counts|    Avg_Magnitude|Max_Magnitude|\n",
      "+----+------+-----------------+-------------+\n",
      "|1990|   196|5.858163265306125|          7.6|\n",
      "|1975|   150| 5.84866666666667|          7.8|\n",
      "|1977|   148|5.757432432432437|          7.6|\n",
      "|2003|   187|5.850802139037435|          7.6|\n",
      "|2007|   211| 5.89099526066351|          8.4|\n",
      "+----+------+-----------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_quake_freq.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c020a031-b1d4-4266-af48-00e4a4a35006",
   "metadata": {},
   "source": [
    "<h3>3.Stockage des Données :</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "596e6db3-8698-4d66-ad0c-a4ad5af92a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Ecrire df_load dans HDFS.\n",
    "\n",
    "df_load = df_load.withColumnRenamed(\"Magnitude Type\", \"Magnitude_Type\")\n",
    "\n",
    "df_load.write.mode(\"overwrite\").parquet(\"hdfs:///user/root/Quake/quakes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b2d2d8e-f8fe-4d58-bf4c-d4624f20a9a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Ecrire df_quake_freq dans HDFS\n",
    "\n",
    "\n",
    "df_quake_freq.write.mode(\"overwrite\").parquet(\"hdfs:///user/root/Quake/quake_freq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04890e5-877d-470b-9a60-3cb6d63c9de0",
   "metadata": {},
   "source": [
    "<h3>4.Modélisation et Analyse :</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "024bf410-5a69-4943-8888-5d0bc1192094",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Date='01/02/1965', Time='13:44:18', Latitude='19.246', Longitude='145.616', Type='Earthquake', Depth='131.6', Depth Error=None, Depth Seismic Stations=None, Magnitude='6', Magnitude Type='MW', Magnitude Error=None, Magnitude Seismic Stations=None, Azimuthal Gap=None, Horizontal Distance=None, Horizontal Error=None, Root Mean Square=None, ID='ISCGEM860706', Source='ISCGEM', Location Source='ISCGEM', Magnitude Source='ISCGEM', Status='Automatic')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Charger le fichier de données de test dans un dataframe.\n",
    "df_test = spark.read.csv(\"hdfs:///user/root/input/database.csv\", header=True)\n",
    "# Aperçu df_test\n",
    "df_test.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0cf61ed0-f79e-4c74-bc2e-56dd8901b476",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 41:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+---------+----------+-----+---------+--------------+------------+----+\n",
      "|      Date|Latitude|Longitude|      Type|Depth|Magnitude|Magnitude_Type|          ID|Year|\n",
      "+----------+--------+---------+----------+-----+---------+--------------+------------+----+\n",
      "|01/02/1965|  19.246|  145.616|Earthquake|131.6|      6.0|            MW|ISCGEM860706|1965|\n",
      "|01/04/1965|   1.863|  127.352|Earthquake| 80.0|      5.8|            MW|ISCGEM860737|1965|\n",
      "|01/05/1965| -20.579| -173.972|Earthquake| 20.0|      6.2|            MW|ISCGEM860762|1965|\n",
      "|01/08/1965| -59.076|  -23.557|Earthquake| 15.0|      5.8|            MW|ISCGEM860856|1965|\n",
      "|01/09/1965|  11.938|  126.427|Earthquake| 15.0|      5.8|            MW|ISCGEM860890|1965|\n",
      "+----------+--------+---------+----------+-----+---------+--------------+------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df_train = spark.read.parquet(\"hdfs:///user/root/Quake/quakes/\")\n",
    "# Aperçu df_train\n",
    "df_train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "befa23d8-b332-4b9e-b40b-3c7b3bd1637d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+---------+---------+-----+\n",
      "|    time|latitude|longitude|Magnitude|depth|\n",
      "+--------+--------+---------+---------+-----+\n",
      "|13:44:18|  19.246|  145.616|        6|131.6|\n",
      "|11:29:49|   1.863|  127.352|      5.8|   80|\n",
      "|18:05:58| -20.579| -173.972|      6.2|   20|\n",
      "|18:49:43| -59.076|  -23.557|      5.8|   15|\n",
      "|13:32:50|  11.938|  126.427|      5.8|   15|\n",
      "+--------+--------+---------+---------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Sélectionner les champs que nous utiliserons et supprimer les champs dont nous n'avons pas besoin.\n",
    "df_test_clean = df_test['time', 'latitude', 'longitude', 'Magnitude', 'depth']\n",
    "# Aperçu  df_test_clean\n",
    "df_test_clean.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dfc489d1-7f7a-452b-896d-1d4c9abae02d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 43:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+---------+---------+-----+\n",
      "|    Date|Latitude|Longitude|Magnitude|Depth|\n",
      "+--------+--------+---------+---------+-----+\n",
      "|13:44:18|  19.246|  145.616|        6|131.6|\n",
      "|11:29:49|   1.863|  127.352|      5.8|   80|\n",
      "|18:05:58| -20.579| -173.972|      6.2|   20|\n",
      "|18:49:43| -59.076|  -23.557|      5.8|   15|\n",
      "|13:32:50|  11.938|  126.427|      5.8|   15|\n",
      "+--------+--------+---------+---------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Renommer les champs.\n",
    "df_test_clean = df_test_clean.withColumnRenamed('time', 'Date')\\\n",
    "    .withColumnRenamed('latitude', 'Latitude')\\\n",
    "    .withColumnRenamed('longitude', 'Longitude')\\\n",
    "    .withColumnRenamed('depth', 'Depth')\n",
    "\n",
    "# Aperçu df_test_clean\n",
    "df_test_clean.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d620c0d2-76d7-42aa-ae2f-96fa080b8dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Latitude: string (nullable = true)\n",
      " |-- Longitude: string (nullable = true)\n",
      " |-- Magnitude: string (nullable = true)\n",
      " |-- Depth: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Aperçu Schema\n",
    "df_test_clean.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bab95e66-947a-42bb-a6b9-3c2e944ced30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir certains champs de chaîne de caractères en champs numériques.\n",
    "df_test_clean = df_test_clean.withColumn('Latitude', df_test_clean['Latitude'].cast(DoubleType()))\\\n",
    "    .withColumn('Longitude', df_test_clean['Longitude'].cast(DoubleType()))\\\n",
    "    .withColumn('Depth', df_test_clean['Depth'].cast(DoubleType()))\\\n",
    "    .withColumn('Magnitude', df_test_clean['Magnitude'].cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5acbbf06-0218-4912-a905-218b6baaf996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Latitude: double (nullable = true)\n",
      " |-- Longitude: double (nullable = true)\n",
      " |-- Magnitude: double (nullable = true)\n",
      " |-- Depth: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test_clean.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "15b520cb-986a-43af-b60a-6e4c11cd21ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer les dataframes d'entraînement et de test.\n",
    "df_testing = df_test_clean['Latitude', 'Longitude', 'Magnitude', 'Depth']\n",
    "df_training = df_train['Latitude', 'Longitude', 'Magnitude', 'Depth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c247c4cd-aff7-4b96-b107-12aeb87fc0f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 44:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+---------+-----+\n",
      "|Latitude|Longitude|Magnitude|Depth|\n",
      "+--------+---------+---------+-----+\n",
      "|  19.246|  145.616|      6.0|131.6|\n",
      "|   1.863|  127.352|      5.8| 80.0|\n",
      "| -20.579| -173.972|      6.2| 20.0|\n",
      "| -59.076|  -23.557|      5.8| 15.0|\n",
      "|  11.938|  126.427|      5.8| 15.0|\n",
      "+--------+---------+---------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Aperçu df_training\n",
    "df_training.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4910a8fa-99dc-489d-8b37-d7182e622dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+---------+-----+\n",
      "|Latitude|Longitude|Magnitude|Depth|\n",
      "+--------+---------+---------+-----+\n",
      "|  19.246|  145.616|      6.0|131.6|\n",
      "|   1.863|  127.352|      5.8| 80.0|\n",
      "| -20.579| -173.972|      6.2| 20.0|\n",
      "| -59.076|  -23.557|      5.8| 15.0|\n",
      "|  11.938|  126.427|      5.8| 15.0|\n",
      "+--------+---------+---------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Aperçu df_testing\n",
    "df_testing.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7736eb5e-8caa-4e86-ba54-f41220f8a3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Supprimer les enregistrements avec des valeurs nulles de nos dataframes.\n",
    "df_testing = df_testing.dropna()\n",
    "df_training = df_training.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2e28a418-0ddf-4813-b0d5-f87e20157575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\n",
      "  Downloading numpy-1.24.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Downloading numpy-1.24.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m22.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:04\u001b[0m\n",
      "Installing collected packages: numpy\n",
      "Successfully installed numpy-1.24.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7659b72a-9e41-48aa-a5af-b0dc2d864327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.24.4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9927891e-d0e0-4a8d-bacf-1e44650a6681",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f774e5b7-5d5e-4060-bf45-2a8de4aaaa70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Sélectionner les caractéristiques à analyser dans notre modèle, puis créer le vecteur de caractéristiques.\n",
    "assembler = VectorAssembler(inputCols=['Latitude', 'Longitude', 'Depth'], outputCol='features')\n",
    "\n",
    "# Créer le modèle\n",
    "model_reg = RandomForestRegressor(featuresCol='features', labelCol='Magnitude')\n",
    "\n",
    "# Chaîner l'assembleur avec le modèle dans une pipeline.\n",
    "pipeline = Pipeline(stages=[assembler, model_reg])\n",
    "\n",
    "# Entraîner le modèle.\n",
    "model = pipeline.fit(df_training)\n",
    "\n",
    "# Faire la prédiction.\n",
    "pred_results = model.transform(df_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "03d109f0-8281-4417-bb12-8c147b76f598",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 60:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+---------+-----+--------------------+-----------------+\n",
      "|Latitude|Longitude|Magnitude|Depth|            features|       prediction|\n",
      "+--------+---------+---------+-----+--------------------+-----------------+\n",
      "|  19.246|  145.616|      6.0|131.6|[19.246,145.616,1...|5.856788614374566|\n",
      "|   1.863|  127.352|      5.8| 80.0|[1.863,127.352,80.0]|5.848285859448586|\n",
      "| -20.579| -173.972|      6.2| 20.0|[-20.579,-173.972...|5.946778356205064|\n",
      "| -59.076|  -23.557|      5.8| 15.0|[-59.076,-23.557,...|5.975950606279386|\n",
      "|  11.938|  126.427|      5.8| 15.0|[11.938,126.427,1...|5.960209013105624|\n",
      "+--------+---------+---------+-----+--------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Aperçu pred_results dataframe\n",
    "pred_results.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "142ec630-b342-41bc-aa1e-19d5b8eabf84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 61:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 0.415346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Évaluer le modèle\n",
    "# rmse doit être inférieur à 0,5 pour que le modèle soit utile\n",
    "evaluator = RegressionEvaluator(labelCol='Magnitude', predictionCol='prediction', metricName='rmse')\n",
    "rmse = evaluator.evaluate(pred_results)\n",
    "print('Root Mean Squared Error (RMSE) on test data = %g' % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ca57098-1d3b-4ee3-b004-09931a5e00f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCréer le dataset de prédiction.\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Créer le dataset de prédiction.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "705be9c7-348b-4912-93e0-fcab29e4e715",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 62:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------------+----+-------------------+\n",
      "|Latitude|Longitude|   Pred_Magnitude|Year|               RMSE|\n",
      "+--------+---------+-----------------+----+-------------------+\n",
      "|  19.246|  145.616|5.856788614374566|2025|0.41534627540254415|\n",
      "|   1.863|  127.352|5.848285859448586|2025|0.41534627540254415|\n",
      "| -20.579| -173.972|5.946778356205064|2025|0.41534627540254415|\n",
      "| -59.076|  -23.557|5.975950606279386|2025|0.41534627540254415|\n",
      "|  11.938|  126.427|5.960209013105624|2025|0.41534627540254415|\n",
      "+--------+---------+-----------------+----+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Créer le dataset de prédiction.\n",
    "df_pred_results = pred_results['Latitude', 'Longitude', 'prediction']\n",
    "\n",
    "# Renommer le champ de prédiction.\n",
    "df_pred_results = df_pred_results.withColumnRenamed('prediction', 'Pred_Magnitude')\n",
    "\n",
    "# Ajouter plus de colonnes\n",
    "df_pred_results = df_pred_results.withColumn('Year', lit(2025))\\\n",
    "    .withColumn('RMSE', lit(rmse))\n",
    "\n",
    "# Aperçu df_pred_results\n",
    "df_pred_results.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fcca6e8e-f3db-4f80-ad3a-f2755c94d15f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Charger le dataset de prediction dans hdfs\n",
    "df_pred_results.write.mode(\"overwrite\").parquet(\"hdfs:///user/root/Quake/pred_results\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5596de6-3278-44ba-a458-7173bed38182",
   "metadata": {},
   "source": [
    "<h3>5.Visualisation des Résultats :</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faec6014-eeea-425b-aaca-3a51cbe8f530",
   "metadata": {},
   "source": [
    "1️⃣ Installer le connecteur Snowflake pour Spark pour transferer la BD de hdfs a snowflake\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "516286a2-9cfd-43f7-8a31-3924c06a3ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sfOptions = {\n",
    "    \"sfURL\" : \"https://vv85366.switzerland-north.azure.snowflakecomputing.com\",\n",
    "    \"sfDatabase\" : \"Quake\",\n",
    "    \"sfSchema\" : \"PUBLIC\",\n",
    "    \"sfWarehouse\" : \"COMPUTE_WH\",\n",
    "    \"sfRole\" : \"ACCOUNTADMIN\",  \n",
    "    \"sfUser\" : \"ADMIN\",\n",
    "    \"sfPassword\" : \"BGKUzh72xKF4aCA\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6746e36f-617b-473f-ac84-4e5e86b914e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Charger la table Quake depuis HDFS\n",
    "quake_df_1 = spark.read.parquet(\"hdfs:///user/root/Quake/quakes\")\n",
    "quake_df_2 = spark.read.parquet(\"hdfs:///user/root/Quake/quake_freq\")\n",
    "quake_df_3 = spark.read.parquet(\"hdfs:///user/root/Quake/pred_results\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b6568f6-186b-48f9-bd3d-2f55bd39989e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-14 15:44:59,083 WARN snowflake.SnowflakeConnectorUtils$: Query pushdown is not supported because you are using Spark 3.2.4 with a connector designed to support Spark 3.0. Either use the version of Spark supported by the connector or install a version of the connector that supports your version of Spark.\n",
      "2025-02-14 15:46:45,788 WARN snowflake.SnowflakeConnectorUtils$: Query pushdown is not supported because you are using Spark 3.2.4 with a connector designed to support Spark 3.0. Either use the version of Spark supported by the connector or install a version of the connector that supports your version of Spark.\n",
      "2025-02-14 15:46:47,155 WARN snowflake.SnowflakeConnectorUtils$: Query pushdown is not supported because you are using Spark 3.2.4 with a connector designed to support Spark 3.0. Either use the version of Spark supported by the connector or install a version of the connector that supports your version of Spark.\n",
      "2025-02-14 15:48:08,961 WARN snowflake.SnowflakeConnectorUtils$: Query pushdown is not supported because you are using Spark 3.2.4 with a connector designed to support Spark 3.0. Either use the version of Spark supported by the connector or install a version of the connector that supports your version of Spark.\n",
      "2025-02-14 15:48:10,198 WARN snowflake.SnowflakeConnectorUtils$: Query pushdown is not supported because you are using Spark 3.2.4 with a connector designed to support Spark 3.0. Either use the version of Spark supported by the connector or install a version of the connector that supports your version of Spark.\n",
      "2025-02-14 15:52:35,862 WARN snowflake.SnowflakeConnectorUtils$: Query pushdown is not supported because you are using Spark 3.2.4 with a connector designed to support Spark 3.0. Either use the version of Spark supported by the connector or install a version of the connector that supports your version of Spark.\n"
     ]
    }
   ],
   "source": [
    "# Transférer la première table a Snowflake \n",
    "quake_df_1.write \\\n",
    "    .format(\"snowflake\") \\\n",
    "    .options(**sfOptions) \\\n",
    "    .option(\"dbtable\", \"quakes\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "\n",
    "# Transférer la deuxième table a Snowflake\n",
    "quake_df_2.write \\\n",
    "    .format(\"snowflake\") \\\n",
    "    .options(**sfOptions) \\\n",
    "    .option(\"dbtable\", \"quake_freq\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "\n",
    "# Transférer la troisième table a snowflake\n",
    "quake_df_3.write \\\n",
    "    .format(\"snowflake\") \\\n",
    "    .options(**sfOptions) \\\n",
    "    .option(\"dbtable\", \"pred_results\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb4c53c-95dd-46d6-8b00-fa5a6e35c713",
   "metadata": {},
   "source": [
    "Enfin, nous analyserons les données en créant un tableau de bord avec Tableau software\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbfbff3-a4ad-49b4-882a-9c1a3d816b95",
   "metadata": {},
   "source": [
    "[Lien vers Tableau ](https://public.tableau.com/app/profile/hind.cherrat/viz/Classeur2_17342840465670/BigData?publish=yes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee55a11-e61c-4ca8-8384-77c75024b9ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
